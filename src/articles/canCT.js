const canCT = `
The question **“can computers think?”** has lingered in my mind since the earliest days of my exploration of artificial intelligence. At first, it felt like a neat scientific riddle — something you could solve with enough logic and code. But the more time I spent building and debugging models, watching them stumble in ways that felt oddly human, I realised this question is deeply personal. It makes me wonder what actually gives thought its weight, and whether intelligence without lived experience can ever be called thinking at all.

I haven’t spent years in research labs, but I’ve spent enough time building systems to see both their flashes of brilliance and their ineptitude. I’ve watched models make decisions, generate answers, and spot patterns I never explicitly programmed. But I’ve also seen them hallucinate with unwavering confidence, overlook the obvious, and piece together responses that sound clever but are hollow inside.

The uncertainty begins with the word **think**.
Human thinking is never just about finding the right answer. It’s woven together with intention, emotion, memory, and all the messy experiences that shape us. When I try to solve a problem, I am influenced by the stories I’ve written, my fears, aspirations, the books I’ve read, and even the people I love. When I set this against a neural network quietly adjusting its weights in response to data, it becomes difficult — a mundane oversimplification — to say that both processes belong to the same category.

Yet I can’t ignore when computers do something uncanny — something that *looks* like thinking.

I remember working on a computer vision project where my YOLOX model confidently labeled a **scooter as a dog**. The mistake was hilarious, but what caught my attention was the *certainty*. The model didn’t pause; it just committed. In another project, I watched a vision-language model do the opposite: it stalled endlessly, giving vague answers even when the object was right there. These moments reminded me of people — sometimes wrong, sometimes unsure, sometimes too sure of themselves. Not because the model had awareness, but because its behaviour accidentally mirrored patterns we see in human thinking.

That surface-level resemblance is powerful. It can almost trick us into believing a system is intelligent in the way we are. But when I look closer, the illusion falls apart.

What looks like **confidence** is just probability. What seems like **hesitation** is just uncertainty in learned patterns.

It doesn’t know it’s wrong.

In fact, it *knows* nothing at all.

This became even clearer when I started working with large language models. Sometimes a model writes something that feels almost empathetic, and for a split second, it seems like there’s a mind behind the words. But then the hallucinations pull us back. The model can invent facts with total confidence, never realizing it’s making things up. Its fluency is a mask; it ***looks*** like reasoning, but underneath, it’s just statistics.

John Searle’s **Chinese Room Argument** feels uncomfortably spot-on here: a system can manipulate symbols and produce flawless answers without understanding a single thing. Critics argue that understanding may occur at the system level — after all, no single neuron “understands” language either. But there’s a difference: *our neurons are embedded in a body that feels pain, forms memories, and exists in the world.* The Chinese Room, like current AI, manipulates symbols in isolation, disconnected from lived reality.

So… **does that count as thinking?**

If thinking requires intention, meaning, or consciousness, then **computers do not think**.
But if thinking is just performing tasks we call intelligent — translation, planning, reasoning — then **computers absolutely do**.

Still, I don’t want to downplay what modern AI can do. I’ve seen models produce results that genuinely surprise me. They discover strategies in reinforcement learning or come up with ideas that feel new. People say it’s “just statistical generalization,” but I’m not entirely convinced that this dismissal captures what’s happening. When a model produces something I didn’t program — something clever, something unexpected — the line between “pattern-matching” and “something new” starts to blur. Maybe it’s all patterns at scale. Or maybe emergence is real, and we’re watching something we don’t yet fully grasp.

The deeper question — whether computers could *ever* be conscious — is one I tread around carefully. So far, nothing suggests machines have even a flicker of subjective experience.

To explain how much they lack, recall a formative memory of your own.
Picture the room, the emotions, the scents, the sounds.

These vivid details are **absent** in machines.
They can copy the *products* of human thought, but not the *feeling* behind it.

They don’t have memories; they have parameters. They don’t have emotions; they have loss functions.

*Real thought, in the human sense, comes from living.*

So, **can computers think?**
My honest answer: they think only in the way they are built to — mechanically, statistically, without introspection. Their “thinking” is powerful enough to solve problems that stump us, but empty enough that it never resembles real thought from the inside.

What my work in AI has taught me is that the value of this question isn’t in the final answer, but in how it forces us to look at ourselves. When we compare human and machine thinking, we see what actually matters in our own minds — why *understanding* differs from *output*, why our mistakes matter, why memory shapes intelligence, and why intention gives thought its weight.

And this matters now more than ever. As AI becomes more capable, we risk forgetting the difference. If we treat machine outputs as equivalent to human understanding, we may stop valuing the things that make our thinking meaningful — the struggle, the doubt, the lived context.

The danger isn’t that machines will become too human.
It’s that **we will become too willing to see them as human**, and in doing so, diminish what our minds actually are.

For now, computers just don’t think the way we do.
That difference isn’t a flaw — it’s a reminder of what makes human thought meaningful.

Maybe the future will blur this line.
Right now, the space between computation and consciousness is still ours alone.

`;
export default canCT;
