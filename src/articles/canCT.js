const canCT = `
The question "can computers think?" has lingered in my mind since the earliest days of my exploration of artificial intelligence. At first, it felt like a neat scientific riddle, something you could solve with enough logic and code. But the more time I spent building and debugging models, watching them stumble in ways that felt oddly human, I realised this question is deeply personal. It makes me wonder what actually gives thought its weight, and whether intelligence without any lived experience can ever be called thinking at all.

I may not have spent years in research labs, but I've spent enough time building systems to see both their flashes of brilliance and their ineptitude. I've watched models make decisions, generate answers, and spot patterns I never explicitly programmed. But I've also seen them hallucinate with unwavering confidence, overlook the obvious, and piece together responses that sound clever but are hollow inside.

The uncertainty begins with the word think. Human thinking is never just about finding the right answer. It's woven together with intention, emotion, memory, and all the messy experiences that shape us. When I try to solve a problem, I am influenced by the stories I’ve written, my fears, aspirations, the books I've read, and even the people I love. When I set this against a neural network quietly adjusting its weights in response to data, it becomes difficult (and a mundane oversimplification) to say that both processes belong to the same category.

Yet I can't ignore when computers do something uncanny — something that really does look like thinking. I remember working on a computer vision project where my YOLOX model confidently labeled a scooter as a dog. The mistake was almost hilarious, but what caught my attention was the certainty. The model didn't pause; it just went for it. In another project, I watched a vision-language model do the opposite: it stalled endlessly, giving vague answers even when the object was right there. These moments reminded me of people — sometimes wrong, sometimes unsure, sometimes too sure of themselves. Not because the model had any awareness, but because its behavior accidentally mirrored patterns we see in human thinking.

That surface-level resemblance is powerful. It can almost trick us into believing a system is intelligent in the way we are. But when I look closer, the illusion falls apart. What looks like confidence is just probability. What seems like hesitation is just uncertainty in learned patterns. It doesn't know it's wrong. In fact, it knows nothing at all.

This became clearer when I started working with large language models. Sometimes, a model will write something that sounds almost empathetic, and for a split second, it feels like there's a mind behind the words. But then the hallucinations pull us back into reality. The model can invent facts with total confidence, never realizing it's making things up. Its fluency is just a mask; it looks like reasoning, but underneath, it's just statistics. John Searle's Chinese Room Argument feels uncomfortably spot-on here: a system can manipulate symbols and produce flawless answers, but still have no idea what any of it means. Critics argue that understanding may occur at the system level—after all, no single neuron "understands" language either. But there's a difference: our neurons are embedded in a body that feels pain, forms memories, and exists in the world. The Chinese Room, like current AI, manipulates symbols in isolation, disconnected from any lived reality.

So does that count as thinking? If thinking needs intention, meaning, or consciousness, then computers do not think. But if it's just about doing the tasks we call intelligent — translation, planning, reasoning — then computers definitely do.

Still, I don't want to downplay what modern AI can do. I've seen models produce results that genuinely surprise me. They discover strategies in reinforcement learning or come up with ideas that feel new. I know people say this is just statistical generalization, but I'm not entirely convinced that plain dismissal captures what's happening. When a model produces something I didn't program, something that solves a problem in an unexpected way, the line between "following learned patterns" and "doing something new" starts to blur. Maybe it's all pattern matching at scale. Or maybe emergence is real, and we're watching something we don't yet fully understand. I don't have an answer, but the fact that I can't easily dismiss these moments tells me the question is harder than it looks.

The deeper question — whether computers could ever be conscious — is one I tread around carefully. So far, nothing I've seen suggests machines have even a flicker of subjective experience. To explain just how much they lack, recall a formative memory. Picture the room where it happened, the emotions you felt, the scents, the sounds. These vivid details are absent in machines. They can copy the products of human thought, but never the feeling behind it. They don't have memories; they just store parameters. They don't have emotions; they only have loss functions. Real thought, in the human sense, comes from living. It comes from stories, losses, childhood rooms, relationships, regrets — all the things that slowly build a mind.

So, can computers think? My honest answer is that they only think in the way they're built to: mechanically, statistically, and without any introspection. Their "thinking" is powerful enough to solve problems that stump us, but empty enough that it never feels like real thought from the inside.

What my work in AI has really taught me is that the value of this question isn't in finding a final answer, but in how it makes us look at ourselves. When we compare human and machine thinking, we start to see what actually matters in our own minds: why understanding is different from output, why our mistakes matter, why memory shapes intelligence, and why intention gives thought its weight. And this matters more than ever, because as AI systems become more capable, we risk forgetting that difference. If we treat machine outputs as equivalent to human understanding, we might stop valuing the things that make our thinking meaningful — the struggle, the doubt, the lived context behind every idea. The danger isn't that machines will become too human, but that we'll become too willing to see them as human, and in doing so, diminish what our own minds actually are.

For now, computers just don't think the way we do. That difference isn't a flaw in machines; it's a reminder of what makes human thought meaningful. Maybe the future will blur this line even more, but right now, the space between computation and consciousness is still ours alone.


`;
export default canCT;
